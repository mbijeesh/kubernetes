Docker Image - package Template from which we can run multiple containers.
Node: Is a virtual or physical mchine where k8s is running. To avoid SPOF, use multiple nodes. So 3 nodes can form a k8s cluster.
Multiple nodes in cluster can share load as well. 
There is a master node in  the cluster which will decide in which worker node a container should run. 
So master node is the orchestration manager. 
K8s - installs :- 
API Server - Running on master node.  To interact with the k8s cluster by user. From the master node it communicate with the kubelet/agent in other worker nodes and carry out 
  the action requested by the worker node. 
etcd -  key value store, to store the information about mutiple nodes in a distributed manner. Will ensure no conflicts with the master. 
scheduler -   Running on master node. Assign the new container to nodes. 
controller  - Running on master node. Take decision when a node goes down and distribute the containers to other nodes. 
Container runtime - Running on the worker node. The container software, in our case the docker.  
kubelet  - the agent runs on each worker node in the cluster. It  ensuer the containers are running on the node as expected. 
# kubectl - command line interface  - can be 1 version difference with cluster. 

kubectl run hello-minikube
kubectl cluster-info
kubectl get nodes
-----
Docker is NOW not a supported  container run time. Though docker can be used for creating pods using containerd. 
ctl - very basic CLI 
nerdctl - docker like cli for 
crictl - CLI fro managing Containers mainly  for debug. Don't use with kubelet in K8S.
-----
Ways to setup/configure k8s:
Minikube  - single node k8s cluster  ( Try https://minikube.sigs.k8s.io/docs/start/)
microk8s 
kubeadm - to set up multi node  k8s cluster in local
ALso available in GCP/AWS/Azure
--
Minikube - comes with all services inside one bundle as a single node k8s cluster
can be download from internet
Then use virtual box. 
--

Before set up  we need to install kubectl 
https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/  
Check virtualization enabled 
grep -E --color 'vmx|svm' /proc/cpuinfo  
Install virtualbox in your ubuntu/windows laptop.
The minikube installation in windows/linux create the VM automatically inside the virtual box.
See documentation 
minikube status
kubectl get nodes
#kubectl creaet deployment <name>  <image path>

#kubectl get deployments

#kubectl expose deploymnet <name> --type --port=
#minikube service name --url    > will give  the URL
#kubectl get pods
Access it, 
Now delete the service:
#kubectl delete services <name>
delet the deploymnet
#kubectl delete deployment <name>
#kubectl get pods ( it shows terminating )

pod is smallest entity that you can manage in k8s
When the load increases,  add  new pod
To again  increase, add a new node in the cluster and add new pod. 
A pod has the one to one relationship with the container running your application. To scale up create new pod and to scale down , just delete the  pod.
No need to add additional container in the pod.

Two containers can be a part of one pod. So that when a  application+supportapplication  need scale up, just add new pod which already contains both containers 
 - application and the support application.
 
 In docker, consider below scenario - 
    For One application one container started, and scaled up to 5 
    Also there is a supportapp container, which also scaled to 5.  Now there is a manual task to map the link between one app to its support app container. 
    But k8s will take this work fo us. 
    
   ----
   kubectl run nginx --image=nginx  (  will download image from docker hub)
   #kubectl getpods
   #kubectl getpods -o wide
   #kubectl describe pod nginx
   Kubernetes Concepts - https://kubernetes.io/docs/concepts/

Also, we can create a yaml file using below command
#kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx.yaml
#kubectl create -f nginx.yaml
# kubectl edit 

Pod Overview- https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/
---
Replica set:
For high availability 
For load balancing/scaling
replicaset-definition.yaml

To Define template of a pod. 
Use the pod definition from metadata and copy as a child under the 
template metadata.
apiVersion: apps/v1
kind: ReplicaSet
metadata: 
  name:my-app-replicaset
  labels:
    app: myapp
    type: dev
spec:
  template:  # from here, till image - just copied from pod definition. 
    metadata:
      name: myapp-prod
      labels:
        app: myapp
        type: dev
    spec: 
      containers:
      - name: nginx
        image: nginx
  replicas: 3
  selector: 
    matchLabels: 
      type: dev

## the above yaml will create a replicaset and which will run 3 pods.
#kubectl create -f replicaset-definition.yaml
#kubectl get replicaset
If we want to scal up, just change the number 3 to 4 and 
#kubectl replace -f replicaset-definition.yaml 
Without modifying the file, to scale temporarily,use below command
#kubectl scale --replicas=6 -f  replicaset-definition.yaml
#kubectl scale --replicas=6  replicaset myapp-replicaset

#kubectl get replicaset
#kubectl delete replicaset myapp-replicaset  (also delete the pods)
In replicaset, if we delete one pod, it will recreate. 
#kubectl edit  > edit the file in memory , not the actual file. So changes applied immediate.
---------------
Deployment
Usefull for rolling update. Rollback is easy. 
Similar to replicaset yaml file, but we use 'kind'  Deployment instead of Replicaset. 
Deployment will create a Replicaset 
template metadata.
apiVersion: apps/v1
kind: Deployment
metadata: 
  name:my-app-replicaset
  labels:
    app: myapp
    type: dev
spec:
  template:  # from here, till image - just copied from pod definition. 
    metadata:
      name: myapp-prod
      labels:
        app: myapp
        type: dev
    spec: 
      containers:
      - name: nginx
        image: nginx
  replicas: 3
  selector: 
    matchLabels: 
      type: dev
============
kubectl create -f deployment.yaml 
kubectl get deployments
kubectl get replicaset
kubectl get pods
kubectl get all 
---------
controlplane ~ âžœ  cat httpd-frontend.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-frontend
  labels:
    app: my-httpd
spec:
  replicas: 3
  selector:
    matchLabels: 
      type: httpd-b
  template:
    metadata: 
      labels:
        name: httpd-test
        type: httpd-b
    spec: 
      containers:
      -  name: httpd
         image: httpd:2.4-alpine

---
kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3
======
Rollout  and versioning
#kubectl rollout status deployment/myapp-deployment
#kubectl rollout history deployment/my-app-deployment
Recreate strategy =>  distroy all pods abd recreate ( downtime)
Rolling update =>  distroy and recreate one pod at a time. (defualt startegy)
After a new rollout, if  there aby error, easily go back to previous version.
#kubectl rollout undo deployment/myapp-deployment
Without changing the definition file,do the rollout example:
#kubectl set image deployment/myapp-deployment nginx=nginx-1.1

To record the command 
#kubectl create -f myapp-deployment.yaml  --record
=============
Networking
10.244.0.0 - assigned to the cluster
Pods will get IP in this range. Containers inside the pods will have internal IPs.

All containers/PODs can communicate to one another without NAT
All nodes can communicate with all containers and vice-versa without NAT. 
K8S does not has a default mechanism to solve the above two conditions.
Other SWs like flannel, calico etc will do the routing using 10.244.0.0 NW
--
Services
To enable the interconnectivity of k8s componenets  as well as the the external communication. 
Nodeport
ClusterIP
LoadBalancer

Nodeport: How it connect and works.
kubernetes node IP:<port>     ( in the range 3000 - 32767)
The 'service' itself has a port and this again connect to the actual port in the pod. 
Ports are in the view from the service. 
So targetport => port in the POD. 
   port => port of the service itself
   nodeport =>  node port/ in which the external connections reach. 
#kubectl get svc
#curl http://ip:nodeport

(Selector is the labels deined for the POD)
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: nodePort
  ports: 
   - targetPort: 80
     port: 80
     nodePort: 30001
  selector:
    app: myapp
    type: front-end

ClusterIP:
PODS grouped based  on the service they are providing (same targetport)

apiVersion: v1
kind: Service
metadata:
  name: back-end
spec:
  type: ClusterIP          #[Cluster IP is the default type]
  ports: 
   - targetPort: 80         #[ POD's port ]
     port: 80
  selector:
    app: myapp
    type: back-end

Loadbalancer:  Better in CLoud platforms - use native LB. 

apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: LoadBanacer
  ports: 
   - targetPort: 80
     port: 80
     nodePort: 30001
---
In the Service 's spec.selector, you can identify which pods to route traffic to only by their labels.
On the other hand, in the Deployment 's spec.selector you have two options to decide on which node the pods
will be scheduled on, which are: matchExpressions, matchLabels.






 







